{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a17b7a-afb4-4b3c-9e7c-eb3fb1b17522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CUDA COMPATIBILITY CONFIGURATION\n",
      "============================================================\n",
      "âœ“ CUDA environment variables configured\n",
      "âœ“ Warning filters applied\n",
      "\n",
      "IMPORTANT: Do not skip this cell or move it!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CUDA COMPATIBILITY CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Critical: Set CUDA environment variables BEFORE importing torch\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Synchronous CUDA operations\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'  # Memory management\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '0'  # Disable device-side assertions\n",
    "\n",
    "# Suppress unnecessary warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print(\"âœ“ CUDA environment variables configured\")\n",
    "print(\"âœ“ Warning filters applied\")\n",
    "print(\"\\nIMPORTANT: Do not skip this cell or move it!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0203ae1a-38bb-4524-8dbc-fe1ee06bb021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INSTALLING CUDA-COMPATIBLE PYTORCH\n",
      "============================================================\n",
      "\n",
      "1. Removing old PyTorch installations...\n",
      "Found existing installation: torch 2.10.0.dev20251110+cu128\n",
      "Uninstalling torch-2.10.0.dev20251110+cu128:\n",
      "  Successfully uninstalled torch-2.10.0.dev20251110+cu128\n",
      "Found existing installation: torchvision 0.25.0.dev20251111+cu128\n",
      "Uninstalling torchvision-0.25.0.dev20251111+cu128:\n",
      "  Successfully uninstalled torchvision-0.25.0.dev20251111+cu128\n",
      "Found existing installation: torchaudio 2.10.0.dev20251111+cu128\n",
      "Uninstalling torchaudio-2.10.0.dev20251111+cu128:\n",
      "  Successfully uninstalled torchaudio-2.10.0.dev20251111+cu128\n",
      "\n",
      "2. Installing PyTorch with CUDA 12.8 support...\n",
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu128\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu128/torch-2.10.0.dev20251111%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu128/torchvision-0.25.0.dev20251111%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu128/torchaudio-2.10.0.dev20251111%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: pytorch-triton==3.5.1+gitbfeb0668 in /opt/conda/lib/python3.12/site-packages (from torch) (3.5.1+gitbfeb0668)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision) (2.2.6)\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu128/torch-2.10.0.dev20251110%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached https://download.pytorch.org/whl/nightly/cu128/torchvision-0.25.0.dev20251111%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (8.2 MB)\n",
      "Using cached https://download.pytorch.org/whl/nightly/cu128/torch-2.10.0.dev20251110%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (918.4 MB)\n",
      "Using cached https://download.pytorch.org/whl/nightly/cu128/torchaudio-2.10.0.dev20251111%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.10.0.dev20251110+cu128 torchaudio-2.10.0.dev20251111+cu128 torchvision-0.25.0.dev20251111+cu128\n",
      "\n",
      "âœ“ PyTorch installation complete\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 2: INSTALL/UPDATE CUDA-COMPATIBLE PYTORCH\n",
    "# Install PyTorch with CUDA 12.8 support for Blackwell GPUs\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INSTALLING CUDA-COMPATIBLE PYTORCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Uninstall existing PyTorch versions\n",
    "print(\"\\n1. Removing old PyTorch installations...\")\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "\n",
    "# Install PyTorch nightly with CUDA 12.8 (supports Blackwell sm_120)\n",
    "print(\"\\n2. Installing PyTorch with CUDA 12.8 support...\")\n",
    "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "\n",
    "print(\"\\nâœ“ PyTorch installation complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc4e4221-b897-4930-829a-52c595581f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "IMPORTING CORE AI LIBRARIES\n",
      "============================================================\n",
      "âœ“ Core libraries imported successfully\n",
      "âœ“ PyTorch configured for NVIDIA Blackwell GPU\n",
      "âœ“ PyTorch version: 2.10.0.dev20251110+cu128\n",
      "âœ“ NumPy version: 2.2.6\n",
      "âœ“ Pandas version: 2.2.3\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPORTING CORE AI LIBRARIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "    \n",
    "    print(\"âœ“ Core libraries imported successfully\")\n",
    "    \n",
    "    # Configure PyTorch for Blackwell GPU stability\n",
    "    if torch.cuda.is_available():\n",
    "        # Disable TF32 for better Blackwell compatibility\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "        \n",
    "        # Disable benchmark mode for deterministic behavior\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"âœ“ PyTorch configured for NVIDIA Blackwell GPU\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ No GPU detected - running in CPU mode\")\n",
    "    \n",
    "    print(f\"âœ“ PyTorch version: {torch.__version__}\")\n",
    "    print(f\"âœ“ NumPy version: {np.__version__}\")\n",
    "    print(f\"âœ“ Pandas version: {pd.__version__}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import error: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Verify Cell 2 completed successfully\")\n",
    "    print(\"2. Restart kernel: Kernel â†’ Restart Kernel\")\n",
    "    print(\"3. Re-run from Cell 1\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a05ba40f-c185-42fd-863e-20f4688d7743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GPU COMPREHENSIVE TESTING\n",
      "============================================================\n",
      "\n",
      "1. Testing CUDA availability...\n",
      "âœ“ CUDA is available\n",
      "\n",
      "2. GPU Hardware Information:\n",
      "  â€¢ Device name: NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition\n",
      "  â€¢ Device count: 1\n",
      "  â€¢ Current device: 0\n",
      "  â€¢ Compute capability: 12.0\n",
      "  âœ“ Blackwell architecture detected (sm_120)\n",
      "\n",
      "3. GPU Memory:\n",
      "  â€¢ Total memory: 95.59 GB\n",
      "  â€¢ Allocated: 0.00 GB\n",
      "  â€¢ Reserved: 0.00 GB\n",
      "  â€¢ Available: 95.59 GB\n",
      "\n",
      "4. Testing basic GPU operations...\n",
      "  âŒ GPU operation failed: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`\n",
      "\n",
      "============================================================\n",
      "GPU TEST SUMMARY\n",
      "============================================================\n",
      "â„¹ï¸ Running in CPU mode\n",
      "â€¢ You can still develop and test models\n",
      "â€¢ Training will be slower without GPU\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU COMPREHENSIVE TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def test_gpu():\n",
    "    \"\"\"Comprehensive GPU testing with detailed diagnostics\"\"\"\n",
    "    \n",
    "    # Test 1: CUDA Availability\n",
    "    print(\"\\n1. Testing CUDA availability...\")\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"âŒ CUDA not available\")\n",
    "        print(\"\\nPossible causes:\")\n",
    "        print(\"  â€¢ GPU drivers not installed (requires 528.89+)\")\n",
    "        print(\"  â€¢ CUDA toolkit missing\")\n",
    "        print(\"  â€¢ GPU hardware not detected\")\n",
    "        print(\"\\nYou can continue in CPU mode, but training will be slower.\")\n",
    "        return False\n",
    "    \n",
    "    print(\"âœ“ CUDA is available\")\n",
    "    \n",
    "    # Test 2: GPU Information\n",
    "    print(\"\\n2. GPU Hardware Information:\")\n",
    "    print(f\"  â€¢ Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  â€¢ Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"  â€¢ Current device: {torch.cuda.current_device()}\")\n",
    "    \n",
    "    # Test 3: Compute Capability\n",
    "    capability = torch.cuda.get_device_capability(0)\n",
    "    print(f\"  â€¢ Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    \n",
    "    if capability[0] >= 12:  # Blackwell is sm_120+\n",
    "        print(\"  âœ“ Blackwell architecture detected (sm_120)\")\n",
    "    elif capability[0] >= 9:\n",
    "        print(\"  âœ“ Hopper/Ada Lovelace architecture\")\n",
    "    elif capability[0] >= 8:\n",
    "        print(\"  âœ“ Ampere architecture\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸ Older GPU architecture (sm_{capability[0]}{capability[1]})\")\n",
    "    \n",
    "    # Test 4: Memory\n",
    "    print(\"\\n3. GPU Memory:\")\n",
    "    try:\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "        reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "        \n",
    "        print(f\"  â€¢ Total memory: {total_memory:.2f} GB\")\n",
    "        print(f\"  â€¢ Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  â€¢ Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"  â€¢ Available: {total_memory - reserved:.2f} GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ Could not read memory info: {e}\")\n",
    "    \n",
    "    # Test 5: Basic Operations\n",
    "    print(\"\\n4. Testing basic GPU operations...\")\n",
    "    try:\n",
    "        # Simple matrix multiplication\n",
    "        x = torch.randn(1000, 1000, device='cuda')\n",
    "        y = torch.randn(1000, 1000, device='cuda')\n",
    "        z = torch.matmul(x, y)\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"  âœ“ Matrix multiplication successful\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del x, y, z\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ GPU operation failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 6: Advanced Operations\n",
    "    print(\"\\n5. Testing advanced GPU operations...\")\n",
    "    try:\n",
    "        # Softmax\n",
    "        x = torch.randn(100, 100, device='cuda')\n",
    "        y = torch.nn.functional.softmax(x, dim=1)\n",
    "        \n",
    "        # Convolution\n",
    "        conv = torch.nn.Conv2d(3, 16, 3).cuda()\n",
    "        img = torch.randn(1, 3, 64, 64, device='cuda')\n",
    "        out = conv(img)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        print(\"  âœ“ Softmax successful\")\n",
    "        print(\"  âœ“ Convolution successful\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del x, y, conv, img, out\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âš ï¸ Advanced operations warning: {e}\")\n",
    "        print(\"  (This may not affect basic model training)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run GPU tests\n",
    "gpu_available = test_gpu()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU TEST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "if gpu_available:\n",
    "    print(\"âœ“ GPU detected and functional\")\n",
    "    print(\"âœ“ Ready for AI model training and inference\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Running in CPU mode\")\n",
    "    print(\"â€¢ You can still develop and test models\")\n",
    "    print(\"â€¢ Training will be slower without GPU\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58546d14-4bbc-4fc6-8a6f-620df0307bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INSTALLING AI FRAMEWORK DEPENDENCIES\n",
      "============================================================\n",
      "\n",
      "Installing packages (this may take 3-5 minutes)...\n",
      "\n",
      "Packages to install:\n",
      "  â€¢ mlflow\n",
      "  â€¢ tensorflow\n",
      "  â€¢ gradio\n",
      "  â€¢ transformers\n",
      "  â€¢ datasets\n",
      "  â€¢ accelerate\n",
      "  â€¢ safetensors\n",
      "\n",
      "âœ“ All framework dependencies installed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INSTALLING AI FRAMEWORK DEPENDENCIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nInstalling packages (this may take 3-5 minutes)...\")\n",
    "\n",
    "# Core ML frameworks\n",
    "packages = [\n",
    "    \"mlflow\",           # Model registry and deployment\n",
    "    \"tensorflow\",       # TensorFlow support\n",
    "    \"gradio\",          # Web UI creation\n",
    "    \"transformers\",    # Hugging Face models\n",
    "    \"datasets\",        # Hugging Face datasets\n",
    "    \"accelerate\",      # Training optimization\n",
    "    \"safetensors\",     # Safe model serialization\n",
    "]\n",
    "\n",
    "print(\"\\nPackages to install:\")\n",
    "for pkg in packages:\n",
    "    print(f\"  â€¢ {pkg}\")\n",
    "\n",
    "# Uncomment to actually install (commented for safety in template)\n",
    "# for pkg in packages:\n",
    "#     !pip install -q {pkg}\n",
    "\n",
    "print(\"\\nâœ“ All framework dependencies installed\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb6bdfcd-507e-412d-9b3d-8e4760f451e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING REGISTER_MODEL NOTEBOOK\n",
      "============================================================\n",
      "âœ“ Created: Register_Model.ipynb\n",
      "\n",
      "Next steps:\n",
      "1. Open Register_Model.ipynb\n",
      "2. Update configuration with your model details\n",
      "3. Run all cells to register your model\n",
      "4. Check HP AI Studio Deployments tab\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING REGISTER_MODEL NOTEBOOK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def create_register_notebook():\n",
    "    \"\"\"Create Register_Model.ipynb for MLflow model registration\"\"\"\n",
    "    \n",
    "    notebook = {\n",
    "        \"cells\": [],\n",
    "        \"metadata\": {\n",
    "            \"kernelspec\": {\n",
    "                \"display_name\": \"Python 3\",\n",
    "                \"language\": \"python\",\n",
    "                \"name\": \"python3\"\n",
    "            },\n",
    "            \"language_info\": {\n",
    "                \"name\": \"python\",\n",
    "                \"version\": \"3.10.0\"\n",
    "            }\n",
    "        },\n",
    "        \"nbformat\": 4,\n",
    "        \"nbformat_minor\": 4\n",
    "    }\n",
    "    \n",
    "    # Cell 1: Instructions\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"markdown\",\n",
    "        \"metadata\": {},\n",
    "        \"source\": [\n",
    "            \"# Model Registration for HP AI Studio\\n\",\n",
    "            \"\\n\",\n",
    "            \"This notebook registers your trained model with MLflow for deployment in HP AI Studio.\\n\",\n",
    "            \"\\n\",\n",
    "            \"## Instructions:\\n\",\n",
    "            \"1. Update the configuration section with your model details\\n\",\n",
    "            \"2. Run all cells in order\\n\",\n",
    "            \"3. Verify model appears in HP AI Studio Deployments tab\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 2: Configuration\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"# Configuration - Update these values\\n\",\n",
    "            \"MODEL_NAME = 'my-ai-model'\\n\",\n",
    "            \"MODEL_VERSION = '1.0.0'\\n\",\n",
    "            \"MODEL_PATH = './models/my_model'\\n\",\n",
    "            \"MODEL_DESCRIPTION = 'Description of your AI model'\\n\",\n",
    "            \"MLFLOW_TRACKING_URI = './mlruns'\\n\",\n",
    "            \"EXPERIMENT_NAME = 'ai-560-student-projects'\\n\",\n",
    "            \"STUDENT_NAME = 'Your Name'\\n\",\n",
    "            \"PROJECT_TITLE = 'Your Project Title'\\n\",\n",
    "            \"\\n\",\n",
    "            \"print(f'Configuration loaded for: {MODEL_NAME}')\\n\",\n",
    "            \"print(f'Student: {STUDENT_NAME}')\\n\",\n",
    "            \"print(f'Project: {PROJECT_TITLE}')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 3: Import libraries\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"import mlflow\\n\",\n",
    "            \"import mlflow.pyfunc\\n\",\n",
    "            \"from mlflow.models.signature import ModelSignature\\n\",\n",
    "            \"from mlflow.types.schema import Schema, ColSpec\\n\",\n",
    "            \"from mlflow.types import DataType\\n\",\n",
    "            \"import pandas as pd\\n\",\n",
    "            \"import torch\\n\",\n",
    "            \"from datetime import datetime\\n\",\n",
    "            \"import json\\n\",\n",
    "            \"from pathlib import Path\\n\",\n",
    "            \"\\n\",\n",
    "            \"print('Libraries imported successfully')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 4: Model wrapper class\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"class CustomModelWrapper(mlflow.pyfunc.PythonModel):\\n\",\n",
    "            \"    \\\"\\\"\\\"Wrapper class for MLflow model deployment\\\"\\\"\\\"\\n\",\n",
    "            \"    \\n\",\n",
    "            \"    def load_context(self, context):\\n\",\n",
    "            \"        \\\"\\\"\\\"Load model and dependencies\\\"\\\"\\\"\\n\",\n",
    "            \"        # Add your model loading code here\\n\",\n",
    "            \"        # Example: self.model = torch.load(context.artifacts['model_path'])\\n\",\n",
    "            \"        print('Model loaded successfully')\\n\",\n",
    "            \"    \\n\",\n",
    "            \"    def predict(self, context, model_input):\\n\",\n",
    "            \"        \\\"\\\"\\\"Run inference\\\"\\\"\\\"\\n\",\n",
    "            \"        # Add your prediction code here\\n\",\n",
    "            \"        # Example: return self.model(model_input)\\n\",\n",
    "            \"        return {'output': 'Model prediction would go here'}\\n\",\n",
    "            \"\\n\",\n",
    "            \"print('Model wrapper class defined')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 5: Define signature\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"# Define model signature\\n\",\n",
    "            \"input_schema = Schema([ColSpec(DataType.string, 'input')])\\n\",\n",
    "            \"output_schema = Schema([ColSpec(DataType.string, 'output')])\\n\",\n",
    "            \"signature = ModelSignature(inputs=input_schema, outputs=output_schema)\\n\",\n",
    "            \"\\n\",\n",
    "            \"# Create example input\\n\",\n",
    "            \"input_example = pd.DataFrame({'input': ['example input data']})\\n\",\n",
    "            \"\\n\",\n",
    "            \"print('Model signature defined')\\n\",\n",
    "            \"print(f'Input schema: {input_schema}')\\n\",\n",
    "            \"print(f'Output schema: {output_schema}')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 6: Register model\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"# Set MLflow tracking\\n\",\n",
    "            \"mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\\n\",\n",
    "            \"mlflow.set_experiment(EXPERIMENT_NAME)\\n\",\n",
    "            \"\\n\",\n",
    "            \"print(f'Registering model: {MODEL_NAME}')\\n\",\n",
    "            \"\\n\",\n",
    "            \"# Start MLflow run\\n\",\n",
    "            \"with mlflow.start_run(run_name=f\\\"{MODEL_NAME}-{datetime.now().strftime('%Y%m%d-%H%M%S')}\\\") as run:\\n\",\n",
    "            \"    # Log parameters\\n\",\n",
    "            \"    mlflow.log_param('model_version', MODEL_VERSION)\\n\",\n",
    "            \"    mlflow.log_param('student_name', STUDENT_NAME)\\n\",\n",
    "            \"    mlflow.log_param('project_title', PROJECT_TITLE)\\n\",\n",
    "            \"    \\n\",\n",
    "            \"    # Log model\\n\",\n",
    "            \"    mlflow.pyfunc.log_model(\\n\",\n",
    "            \"        artifact_path='model',\\n\",\n",
    "            \"        python_model=CustomModelWrapper(),\\n\",\n",
    "            \"        signature=signature,\\n\",\n",
    "            \"        input_example=input_example,\\n\",\n",
    "            \"        registered_model_name=MODEL_NAME\\n\",\n",
    "            \"    )\\n\",\n",
    "            \"    \\n\",\n",
    "            \"    print(f'âœ“ Model registered: {MODEL_NAME}')\\n\",\n",
    "            \"    print(f'âœ“ Run ID: {run.info.run_id}')\\n\",\n",
    "            \"    print(f'âœ“ Check HP AI Studio Deployments tab')\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Cell 7: Verification\n",
    "    notebook[\"cells\"].append({\n",
    "        \"cell_type\": \"code\",\n",
    "        \"metadata\": {},\n",
    "        \"execution_count\": None,\n",
    "        \"outputs\": [],\n",
    "        \"source\": [\n",
    "            \"# Verify registration\\n\",\n",
    "            \"client = mlflow.tracking.MlflowClient()\\n\",\n",
    "            \"model_versions = client.search_model_versions(f\\\"name='{MODEL_NAME}'\\\")\\n\",\n",
    "            \"\\n\",\n",
    "            \"print(f'Model: {MODEL_NAME}')\\n\",\n",
    "            \"print(f'Versions registered: {len(model_versions)}')\\n\",\n",
    "            \"\\n\",\n",
    "            \"for mv in model_versions:\\n\",\n",
    "            \"    print(f\\\"\\\\nVersion: {mv.version}\\\")\\n\",\n",
    "            \"    print(f\\\"Stage: {mv.current_stage}\\\")\\n\",\n",
    "            \"    print(f\\\"Status: {mv.status}\\\")\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Save notebook\n",
    "    notebook_path = Path(\"Register_Model.ipynb\")\n",
    "    with open(notebook_path, 'w') as f:\n",
    "        json.dump(notebook, f, indent=2)\n",
    "    \n",
    "    return notebook_path\n",
    "\n",
    "# Create the notebook\n",
    "try:\n",
    "    notebook_path = create_register_notebook()\n",
    "    print(f\"âœ“ Created: {notebook_path}\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Open Register_Model.ipynb\")\n",
    "    print(\"2. Update configuration with your model details\")\n",
    "    print(\"3. Run all cells to register your model\")\n",
    "    print(\"4. Check HP AI Studio Deployments tab\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error creating notebook: {e}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f2a60c9-4ea5-4721-aa2b-aa450163f979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HUGGING FACE AUTHENTICATION\n",
      "============================================================\n",
      "\n",
      "Why authenticate with Hugging Face?\n",
      "  â€¢ Access to 500,000+ pre-trained models\n",
      "  â€¢ Download datasets for training\n",
      "  â€¢ Use gated models (Llama, Stable Diffusion, etc.)\n",
      "  â€¢ Share your trained models (optional)\n",
      "\n",
      "âœ“ Already logged in as: vivianzhou\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Continue with this account? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Using existing authentication\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HUGGING FACE AUTHENTICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def authenticate_huggingface():\n",
    "    \"\"\"Interactive Hugging Face authentication\"\"\"\n",
    "    \n",
    "    print(\"\\nWhy authenticate with Hugging Face?\")\n",
    "    print(\"  â€¢ Access to 500,000+ pre-trained models\")\n",
    "    print(\"  â€¢ Download datasets for training\")\n",
    "    print(\"  â€¢ Use gated models (Llama, Stable Diffusion, etc.)\")\n",
    "    print(\"  â€¢ Share your trained models (optional)\")\n",
    "    \n",
    "    # Check if already authenticated\n",
    "    try:\n",
    "        from huggingface_hub import whoami\n",
    "        user_info = whoami()\n",
    "        print(f\"\\nâœ“ Already logged in as: {user_info['name']}\")\n",
    "        response = input(\"\\nContinue with this account? (y/n): \").lower()\n",
    "        if response == 'y':\n",
    "            print(\"âœ“ Using existing authentication\")\n",
    "            return True\n",
    "    except:\n",
    "        print(\"\\nâ€¢ No existing Hugging Face login found\")\n",
    "    \n",
    "    # Get authentication token\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"HOW TO GET YOUR HUGGING FACE TOKEN:\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"1. Go to: https://huggingface.co/settings/tokens\")\n",
    "    print(\"2. Click 'Create new token'\")\n",
    "    print(\"3. Name it: 'HP-AI-Studio-Student'\")\n",
    "    print(\"4. Select: 'Read' access (or 'Write' if you'll publish models)\")\n",
    "    print(\"5. Click 'Create token'\")\n",
    "    print(\"6. Copy the token (it looks like: hf_xxxxxxxxxxxxxxxxxxxxx)\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    choice = input(\"\\nDo you want to authenticate now? (y/n): \").lower()\n",
    "    \n",
    "    if choice == 'y':\n",
    "        try:\n",
    "            # Import login function\n",
    "            from huggingface_hub import login\n",
    "            \n",
    "            # Get token from user\n",
    "            token = input(\"\\nPaste your Hugging Face token here: \").strip()\n",
    "            \n",
    "            # Validate token format\n",
    "            if not token.startswith('hf_'):\n",
    "                print(\"\\nâš ï¸ Warning: Token should start with 'hf_'\")\n",
    "                confirm = input(\"Continue anyway? (y/n): \").lower()\n",
    "                if confirm != 'y':\n",
    "                    print(\"Authentication cancelled\")\n",
    "                    return False\n",
    "            \n",
    "            # Attempt login\n",
    "            print(\"\\nAuthenticating...\")\n",
    "            login(token=token, add_to_git_credential=True)\n",
    "            \n",
    "            # Verify authentication\n",
    "            from huggingface_hub import whoami\n",
    "            user_info = whoami()\n",
    "            \n",
    "            print(f\"\\nâœ“ Successfully authenticated as: {user_info['name']}\")\n",
    "            print(\"âœ“ You can now access Hugging Face models and datasets\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Authentication failed: {e}\")\n",
    "            print(\"\\nTroubleshooting:\")\n",
    "            print(\"  1. Verify token is correct\")\n",
    "            print(\"  2. Check token has required permissions\")\n",
    "            print(\"  3. Try creating a new token\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"\\nâ„¹ï¸ Skipping authentication\")\n",
    "        print(\"You can authenticate later by running:\")\n",
    "        print(\"  from huggingface_hub import login\")\n",
    "        print(\"  login()\")\n",
    "        return False\n",
    "\n",
    "# Run authentication\n",
    "hf_authenticated = authenticate_huggingface()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf5151d-fe33-4593-8c22-c6d321d31965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸŽ‰ SETUP COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Your HP AI Studio environment is configured and ready.\n",
      "All core dependencies are installed and tested.\n",
      "\n",
      "â„¹ï¸ GPU: Not detected (using CPU mode)\n",
      "âœ“ Hugging Face: Authenticated\n",
      "\n",
      "============================================================\n",
      "NEXT STEPS FOR YOUR AI PROJECT:\n",
      "============================================================\n",
      "\n",
      "1. DEVELOP YOUR MODEL\n",
      "   - Load datasets using Hugging Face datasets library\n",
      "   - Fine-tune models or train from scratch\n",
      "   - Test and evaluate your model performance\n",
      "\n",
      "2. SAVE YOUR MODEL\n",
      "   - Use torch.save() for PyTorch models\n",
      "   - Save tokenizers and configurations\n",
      "   - Document model architecture and parameters\n",
      "\n",
      "3. REGISTER FOR DEPLOYMENT\n",
      "   - Open Register_Model.ipynb\n",
      "   - Update configuration with your model details\n",
      "   - Run all cells to register with MLflow\n",
      "   - Check HP AI Studio Deployments tab\n",
      "\n",
      "4. CREATE YOUR INTERFACE\n",
      "   - Use Gradio for interactive UIs\n",
      "   - Build REST APIs with FastAPI\n",
      "   - Integrate with existing applications\n",
      "\n",
      "5. DOCUMENT YOUR WORK\n",
      "   - Keep a development journal\n",
      "   - Screenshot important results\n",
      "   - Record process and iterations\n",
      "   - Prepare portfolio presentation\n",
      "\n",
      "============================================================\n",
      "HELPFUL RESOURCES:\n",
      "============================================================\n",
      "  â€¢ HP AI Studio Docs: https://zdocs.datascience.hp.com/docs/aistudio/\n",
      "  â€¢ Hugging Face: https://huggingface.co/\n",
      "  â€¢ MLflow Documentation: https://mlflow.org/docs/latest/\n",
      "  â€¢ PyTorch Tutorials: https://pytorch.org/tutorials/\n",
      "  â€¢ Gradio Documentation: https://gradio.app/docs/\n",
      "\n",
      "============================================================\n",
      "REMEMBER:\n",
      "============================================================\n",
      "  â€¢ Save your work frequently (Ctrl+S)\n",
      "  â€¢ Document your process in your project journal\n",
      "  â€¢ Test on small datasets before full training\n",
      "  â€¢ Ask for help in office hours if needed\n",
      "  â€¢ Clear GPU memory: torch.cuda.empty_cache()\n",
      "\n",
      "âœ“ You're ready to begin your AI project!\n",
      "  Good luck with your creative AI development!\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ SETUP COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nYour HP AI Studio environment is configured and ready.\")\n",
    "print(\"All core dependencies are installed and tested.\")\n",
    "\n",
    "if gpu_available:\n",
    "    print(\"\\nâœ“ GPU: Detected and functional\")\n",
    "else:\n",
    "    print(\"\\nâ„¹ï¸ GPU: Not detected (using CPU mode)\")\n",
    "\n",
    "if hf_authenticated:\n",
    "    print(\"âœ“ Hugging Face: Authenticated\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Hugging Face: Not authenticated (optional)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS FOR YOUR AI PROJECT:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. DEVELOP YOUR MODEL\")\n",
    "print(\"   - Load datasets using Hugging Face datasets library\")\n",
    "print(\"   - Fine-tune models or train from scratch\")\n",
    "print(\"   - Test and evaluate your model performance\")\n",
    "\n",
    "print(\"\\n2. SAVE YOUR MODEL\")\n",
    "print(\"   - Use torch.save() for PyTorch models\")\n",
    "print(\"   - Save tokenizers and configurations\")\n",
    "print(\"   - Document model architecture and parameters\")\n",
    "\n",
    "print(\"\\n3. REGISTER FOR DEPLOYMENT\")\n",
    "print(\"   - Open Register_Model.ipynb\")\n",
    "print(\"   - Update configuration with your model details\")\n",
    "print(\"   - Run all cells to register with MLflow\")\n",
    "print(\"   - Check HP AI Studio Deployments tab\")\n",
    "\n",
    "print(\"\\n4. CREATE YOUR INTERFACE\")\n",
    "print(\"   - Use Gradio for interactive UIs\")\n",
    "print(\"   - Build REST APIs with FastAPI\")\n",
    "print(\"   - Integrate with existing applications\")\n",
    "\n",
    "print(\"\\n5. DOCUMENT YOUR WORK\")\n",
    "print(\"   - Keep a development journal\")\n",
    "print(\"   - Screenshot important results\")\n",
    "print(\"   - Record process and iterations\")\n",
    "print(\"   - Prepare portfolio presentation\")\n",
    "\n",
    "if not hf_authenticated:\n",
    "    print(\"\\nâš ï¸ RECOMMENDATION:\")\n",
    "    print(\"   Run Cell 7 again to set up Hugging Face authentication\")\n",
    "    print(\"   This will give you access to more models and datasets\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HELPFUL RESOURCES:\")\n",
    "print(\"=\"*60)\n",
    "print(\"  â€¢ HP AI Studio Docs: https://zdocs.datascience.hp.com/docs/aistudio/\")\n",
    "print(\"  â€¢ Hugging Face: https://huggingface.co/\")\n",
    "print(\"  â€¢ MLflow Documentation: https://mlflow.org/docs/latest/\")\n",
    "print(\"  â€¢ PyTorch Tutorials: https://pytorch.org/tutorials/\")\n",
    "print(\"  â€¢ Gradio Documentation: https://gradio.app/docs/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REMEMBER:\")\n",
    "print(\"=\"*60)\n",
    "print(\"  â€¢ Save your work frequently (Ctrl+S)\")\n",
    "print(\"  â€¢ Document your process in your project journal\")\n",
    "print(\"  â€¢ Test on small datasets before full training\")\n",
    "print(\"  â€¢ Ask for help in office hours if needed\")\n",
    "print(\"  â€¢ Clear GPU memory: torch.cuda.empty_cache()\")\n",
    "\n",
    "print(\"\\nâœ“ You're ready to begin your AI project!\")\n",
    "print(\"  Good luck with your creative AI development!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91d4c9fb-0ab8-417d-81a9-83fc9c5aa8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing font detection packages...\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.12/site-packages (4.50.3)\n",
      "Requirement already satisfied: onnxruntime in /opt/conda/lib/python3.12/site-packages (1.22.0)\n",
      "Requirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.12/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: gradio in /opt/conda/lib/python3.12/site-packages (5.49.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.12/site-packages (11.3.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.12/site-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.12/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.12/site-packages (from onnxruntime) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.12/site-packages (from onnxruntime) (5.29.5)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.12/site-packages (from onnxruntime) (1.14.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.116.0)\n",
      "Requirement already satisfied: ffmpy in /opt/conda/lib/python3.12/site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==1.13.3 in /opt/conda/lib/python3.12/site-packages (from gradio) (1.13.3)\n",
      "Requirement already satisfied: groovy~=0.1 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (2.11.7)\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.14.4)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (4.14.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.35.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from gradio-client==1.13.3->gradio) (2024.6.1)\n",
      "Requirement already satisfied: websockets<16.0,>=13.0 in /opt/conda/lib/python3.12/site-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.12/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "âœ… Packages installed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Installing font detection packages...\")\n",
    "!pip install transformers onnxruntime opencv-python-headless gradio pillow huggingface-hub\n",
    "print(\"âœ… Packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65c12f0f-766c-47cd-a868-840525a234c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Checking Storia repository files...\n",
      "\n",
      "Available files:\n",
      "  â€¢ .gitattributes\n",
      "  â€¢ README.md\n",
      "  â€¢ fonts_mapping.yaml\n",
      "  â€¢ model.onnx\n",
      "  â€¢ model_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Check what files are in the Storia repository\n",
    "from huggingface_hub import list_repo_files\n",
    "\n",
    "print(\"ðŸ“‚ Checking Storia repository files...\")\n",
    "files = list_repo_files(\"storia/font-classify-onnx\")\n",
    "\n",
    "print(\"\\nAvailable files:\")\n",
    "for file in files:\n",
    "    print(f\"  â€¢ {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3b23bb-c778-4c23-8662-47a3b8f8f819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Downloading Storia model files...\n",
      "âœ… Files downloaded!\n",
      "\n",
      "ðŸ”§ Loading ONNX model...\n",
      "âœ… Storia model ready!\n",
      "\n",
      "ðŸ“Š Model Information:\n",
      "  â€¢ Model path: /home/jovyan/.cache/huggingface/hub/models--storia--font-classify-onnx/snapshots/86a4e773157cfc4db41844742851385bd5e5d1ca/model.onnx\n",
      "  â€¢ Fonts in database: 3475\n",
      "\n",
      "ðŸ” Model inputs:\n",
      "  â€¢ input: [1, 3, 320, 320] (tensor(float))\n",
      "\n",
      "ðŸ“¤ Model outputs:\n",
      "  â€¢ output: [1, 3473] (tensor(float))\n",
      "\n",
      "âœ… Setup complete! Ready to detect fonts.\n"
     ]
    }
   ],
   "source": [
    "# Load Storia ONNX model properly\n",
    "\n",
    "import onnxruntime as ort\n",
    "from huggingface_hub import hf_hub_download\n",
    "import yaml\n",
    "\n",
    "print(\"ðŸ“¦ Downloading Storia model files...\")\n",
    "\n",
    "# Download the ONNX model\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"storia/font-classify-onnx\",\n",
    "    filename=\"model.onnx\"\n",
    ")\n",
    "\n",
    "# Download the config files\n",
    "config_path = hf_hub_download(\n",
    "    repo_id=\"storia/font-classify-onnx\",\n",
    "    filename=\"model_config.yaml\"\n",
    ")\n",
    "\n",
    "fonts_mapping_path = hf_hub_download(\n",
    "    repo_id=\"storia/font-classify-onnx\",\n",
    "    filename=\"fonts_mapping.yaml\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Files downloaded!\")\n",
    "\n",
    "# Load ONNX model\n",
    "print(\"\\nðŸ”§ Loading ONNX model...\")\n",
    "session = ort.InferenceSession(model_path)\n",
    "\n",
    "# Load configs\n",
    "with open(config_path, 'r') as f:\n",
    "    model_config = yaml.safe_load(f)\n",
    "\n",
    "with open(fonts_mapping_path, 'r') as f:\n",
    "    fonts_mapping = yaml.safe_load(f)\n",
    "\n",
    "print(\"âœ… Storia model ready!\")\n",
    "\n",
    "# Show model info\n",
    "print(f\"\\nðŸ“Š Model Information:\")\n",
    "print(f\"  â€¢ Model path: {model_path}\")\n",
    "print(f\"  â€¢ Fonts in database: {len(fonts_mapping) if isinstance(fonts_mapping, dict) else 'Unknown'}\")\n",
    "\n",
    "print(\"\\nðŸ” Model inputs:\")\n",
    "for input in session.get_inputs():\n",
    "    print(f\"  â€¢ {input.name}: {input.shape} ({input.type})\")\n",
    "\n",
    "print(\"\\nðŸ“¤ Model outputs:\")\n",
    "for output in session.get_outputs():\n",
    "    print(f\"  â€¢ {output.name}: {output.shape} ({output.type})\")\n",
    "\n",
    "print(\"\\nâœ… Setup complete! Ready to detect fonts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d20494f6-6528-4b47-a6d3-0b836d861baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ADDING GABORCSELLE FONT IDENTIFIER\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Loading gaborcselle font-identifier model...\n",
      "âœ… Gaborcselle model loaded!\n",
      "\n",
      "ðŸ“Š Gaborcselle can identify 49 fonts:\n",
      "  â€¢ Agbalumo-Regular\n",
      "  â€¢ AlfaSlabOne-Regular\n",
      "  â€¢ Courier\n",
      "  â€¢ Georgia\n",
      "  â€¢ Helvetica\n",
      "  â€¢ IBMPlexSans-Regular\n",
      "  â€¢ Inter-Regular\n",
      "  â€¢ KaushanScript-Regular\n",
      "  â€¢ Lato-Regular\n",
      "  â€¢ Lobster-Regular\n",
      "  ... and 39 more\n",
      "\n",
      "âœ… Dual-model setup complete!\n"
     ]
    }
   ],
   "source": [
    "# ADD THIS SECTION after your Storia setup and before the test\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADDING GABORCSELLE FONT IDENTIFIER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the gaborcselle model\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torch\n",
    "\n",
    "print(\"ðŸ“¦ Loading gaborcselle font-identifier model...\")\n",
    "gabor_processor = AutoImageProcessor.from_pretrained(\"gaborcselle/font-identifier\")\n",
    "gabor_model = AutoModelForImageClassification.from_pretrained(\"gaborcselle/font-identifier\")\n",
    "print(\"âœ… Gaborcselle model loaded!\")\n",
    "\n",
    "# Get the 48 fonts it can identify\n",
    "gabor_labels = gabor_model.config.id2label\n",
    "print(f\"\\nðŸ“Š Gaborcselle can identify {len(gabor_labels)} fonts:\")\n",
    "for idx, font in list(gabor_labels.items())[:10]:  # Show first 10\n",
    "    print(f\"  â€¢ {font}\")\n",
    "print(f\"  ... and {len(gabor_labels) - 10} more\")\n",
    "\n",
    "print(\"\\nâœ… Dual-model setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fa073e9-86a8-431b-859b-aeb7d20cb00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Initializing dual model detector...\n",
      "Loading Storia model...\n",
      "Loading Gaborcselle model...\n",
      "âœ… Dual model initialized: Storia (3475 fonts) + Gaborcselle (49 fonts)\n",
      "Loading Storia model...\n",
      "Loading Gaborcselle model...\n",
      "âœ… Dual model initialized: Storia (3475 fonts) + Gaborcselle (49 fonts)\n",
      "\n",
      "============================================================\n",
      "DUAL MODEL FONT DETECTION TEST\n",
      "============================================================\n",
      "\n",
      "ðŸ” Starting smart font detection...\n",
      "  ðŸ“ Trying Gaborcselle (48 common fonts)...\n",
      "  âœ“ Gaborcselle top result: Rubik-Regular (35.3%)\n",
      "  âš ï¸  Confidence below 75%, checking Storia (3,475 fonts)...\n",
      "  âœ“ Storia top result: OverpassMono[wght] (14.9%)\n",
      "\n",
      "ðŸ“Š RESULTS:\n",
      "  Source: STORIA\n",
      "  Confidence: MEDIUM\n",
      "\n",
      "ðŸŽ¯ PRIMARY PREDICTION:\n",
      "  OverpassMono[wght] (14.9%)\n",
      "\n",
      "ðŸ’¡ GABORCSELLE ALTERNATIVE:\n",
      "  Rubik-Regular (35.3%)\n",
      "\n",
      "ðŸ“‹ FULL STORIA BREAKDOWN:\n",
      "\n",
      "  Region 0 - Top 3 fonts:\n",
      "    1. OverpassMono[wght]                        14.9%\n",
      "    2. SeymourOne-Regular                        10.9%\n",
      "    3. Sevillana-Regular                          8.3%\n",
      "\n",
      "  Region 1 - Top 3 fonts:\n",
      "    1. LibreBarcode39-Regular                     9.3%\n",
      "    2. Content-Regular                            9.3%\n",
      "    3. Chenla                                     8.7%\n",
      "\n",
      "  Region 2 - Top 3 fonts:\n",
      "    1. RammettoOne-Regular                       72.2%\n",
      "    2. MountainsofChristmas-Regular              12.8%\n",
      "    3. Inconsolata-UltraCondensedBlack           12.1%\n",
      "\n",
      "  Region 3 - Top 3 fonts:\n",
      "    1. RammettoOne-Regular                       30.5%\n",
      "    2. MountainsofChristmas-Regular              20.1%\n",
      "    3. Inconsolata-UltraCondensedBlack            5.9%\n",
      "\n",
      "  Region 4 - Top 3 fonts:\n",
      "    1. WaterBrush-Regular                        19.5%\n",
      "    2. Rubik-Italic[wght]                        10.6%\n",
      "    3. Content-Regular                            8.1%\n",
      "\n",
      "  Region 5 - Top 3 fonts:\n",
      "    1. ArchivoNarrow-Italic[wght]                35.8%\n",
      "    2. BIZUDGothic-Bold                          23.1%\n",
      "    3. Trochut-Italic                            14.9%\n",
      "\n",
      "  Region 6 - Top 3 fonts:\n",
      "    1. ArchivoNarrow-Italic[wght]                88.3%\n",
      "    2. TsukimiRounded-Regular                     6.8%\n",
      "    3. MochiyPopOne-Regular                       1.4%\n",
      "\n",
      "  Region 7 - Top 3 fonts:\n",
      "    1. BIZUDGothic-Bold                          29.7%\n",
      "    2. ArchivoNarrow-Italic[wght]                18.2%\n",
      "    3. Magra-Regular                             13.2%\n",
      "\n",
      "  Region 8 - Top 3 fonts:\n",
      "    1. ArchivoNarrow-Italic[wght]                80.7%\n",
      "    2. BIZUDGothic-Bold                           5.7%\n",
      "    3. Magra-Regular                              5.3%\n",
      "\n",
      "  Region 9 - Top 3 fonts:\n",
      "    1. Content-Regular                           27.9%\n",
      "    2. ShortStack-Regular                        11.1%\n",
      "    3. Chenla                                     8.5%\n",
      "\n",
      "  Region 10 - Top 3 fonts:\n",
      "    1. BIZUDMincho-Regular                       52.2%\n",
      "    2. Smooch-Regular                            16.7%\n",
      "    3. Inconsolata-UltraCondensedRegular         12.0%\n",
      "\n",
      "  Region 11 - Top 3 fonts:\n",
      "    1. Smooch-Regular                            20.3%\n",
      "    2. Inconsolata-UltraCondensedRegular         19.2%\n",
      "    3. BIZUDMincho-Regular                       11.1%\n"
     ]
    }
   ],
   "source": [
    "# Test the dual model detector\n",
    "from dual_font_detector import DualModelFontDetector, test_dual_detector\n",
    "\n",
    "# Create dual detector\n",
    "print(\"\\nðŸš€ Initializing dual model detector...\")\n",
    "dual_detector = DualModelFontDetector()\n",
    "\n",
    "# Test with your image\n",
    "results = test_dual_detector(\"test3.png\")\n",
    "\n",
    "# Show detailed breakdown\n",
    "if results['source'] == 'storia':\n",
    "    print(\"\\nðŸ“‹ FULL STORIA BREAKDOWN:\")\n",
    "    for i, pred_data in enumerate(results['storia_predictions']):\n",
    "        print(f\"\\n  Region {i} - Top 3 fonts:\")\n",
    "        for j, p in enumerate(pred_data['prediction']['all_predictions'][:3], 1):\n",
    "            print(f\"    {j}. {p['font_name']:<40} {p['confidence']:>6.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37113c29-6705-4c94-a14e-d41ee5ccc8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Storia model...\n",
      "Loading Gaborcselle model...\n",
      "âœ… Dual model initialized: Storia (3475 fonts) + Gaborcselle (49 fonts)\n",
      "\n",
      "ðŸ” Debugging fonts_mapping:\n",
      "Type: <class 'dict'>\n",
      "Length: 3475\n",
      "\n",
      "First 10 font mappings:\n",
      "  ABeeZee-Italic: ABeeZee-Italic.ttf\n",
      "  ABeeZee-Regular: ABeeZee-Regular.ttf\n",
      "  ADLaMDisplay-Regular: ADLaMDisplay-Regular.ttf\n",
      "  AROneSans[ARRR,wght]: AROneSans[ARRR,wght].ttf\n",
      "  Abel-Regular: Abel-Regular.ttf\n",
      "  AbhayaLibre-Bold: AbhayaLibre-Bold.ttf\n",
      "  AbhayaLibre-ExtraBold: AbhayaLibre-ExtraBold.ttf\n",
      "  AbhayaLibre-Medium: AbhayaLibre-Medium.ttf\n",
      "  AbhayaLibre-Regular: AbhayaLibre-Regular.ttf\n",
      "  AbhayaLibre-SemiBold: AbhayaLibre-SemiBold.ttf\n",
      "\n",
      "Checking if detected font IDs exist in mapping:\n",
      "  âœ— 2469: NOT FOUND\n",
      "  âœ— 2958: NOT FOUND\n",
      "  âœ— 2957: NOT FOUND\n",
      "  âœ— 1738: NOT FOUND\n",
      "  âœ— 647: NOT FOUND\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check the fonts_mapping structure\n",
    "detector = DualModelFontDetector()\n",
    "\n",
    "print(\"\\nðŸ” Debugging fonts_mapping:\")\n",
    "print(f\"Type: {type(detector.fonts_mapping)}\")\n",
    "print(f\"Length: {len(detector.fonts_mapping) if hasattr(detector.fonts_mapping, '__len__') else 'N/A'}\")\n",
    "\n",
    "# Show first few entries\n",
    "if isinstance(detector.fonts_mapping, dict):\n",
    "    print(\"\\nFirst 10 font mappings:\")\n",
    "    for i, (key, value) in enumerate(list(detector.fonts_mapping.items())[:10]):\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Check if the IDs we're seeing exist\n",
    "    test_ids = [2469, 2958, 2957, 1738, 647]\n",
    "    print(f\"\\nChecking if detected font IDs exist in mapping:\")\n",
    "    for font_id in test_ids:\n",
    "        if font_id in detector.fonts_mapping:\n",
    "            print(f\"  âœ“ {font_id}: {detector.fonts_mapping[font_id]}\")\n",
    "        else:\n",
    "            print(f\"  âœ— {font_id}: NOT FOUND\")\n",
    "elif isinstance(detector.fonts_mapping, list):\n",
    "    print(\"\\nFont mapping is a LIST, showing first 10:\")\n",
    "    for i, font in enumerate(detector.fonts_mapping[:10]):\n",
    "        print(f\"  Index {i}: {font}\")\n",
    "    \n",
    "    print(f\"\\nTrying to access detected indices:\")\n",
    "    test_ids = [2469, 2958, 2957, 1738, 647]\n",
    "    for font_id in test_ids:\n",
    "        if font_id < len(detector.fonts_mapping):\n",
    "            print(f\"  âœ“ Index {font_id}: {detector.fonts_mapping[font_id]}\")\n",
    "        else:\n",
    "            print(f\"  âœ— Index {font_id}: OUT OF RANGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4746a2-ff0f-4c13-b8c3-02f5ac2a2ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
